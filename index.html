<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Squeezenet with Deep Compression by songhan</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Squeezenet with Deep Compression</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/songhan/SqueezeNet-Deep-Compression" class="btn">View on GitHub</a>
      <a href="https://github.com/songhan/SqueezeNet-Deep-Compression/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/songhan/SqueezeNet-Deep-Compression/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h1>

<p>This is the 660KB compressed SqueezeNet, which is 363x smaller as AlexNet but has the same accuracy as AlexNet. </p>

<p>(There is an even smaller version which is only 470KB. It requires some effort to materialize since each weight is 6-bits.)</p>

<h1>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Usage</h1>

<pre><code>export CAFFE_ROOT=$your_caffe_root

python decode.py /ABSOLUTE_PATH_TO/SqueezeNet_deploy.prototxt /ABSOLUTE_PATH_TO/compressed_SqueezeNet.net /ABSOLUTE_PATH_TO/decompressed_SqueezeNet.caffemodel

note: decompressed_SqueezeNet.caffemodel is the output, can be any name.

$CAFFE_ROOT/build/tools/caffe test --model=SqueezeNet_trainval.prototxt --weights=decompressed_SqueezeNet.caffemodel --iterations=1000 --gpu 0
</code></pre>

<h1>
<a id="related-squeezenet-repo" class="anchor" href="#related-squeezenet-repo" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related SqueezeNet repo</h1>

<p><a href="https://github.com/DeepScale/SqueezeNet">SqueezeNet</a></p>

<p><a href="https://github.com/songhan/SqueezeNet-Deep-Compression">SqueezeNet-Deep-Compression</a></p>

<p><a href="https://github.com/songhan/SqueezeNet-Generator">SqueezeNet-Generator</a></p>

<p><a href="https://github.com/songhan/SqueezeNet-DSD-Training">SqueezeNet-DSD-Training</a></p>

<p><a href="https://github.com/songhan/SqueezeNet-Residual">SqueezeNet-Residual</a></p>

<h1>
<a id="related-papers" class="anchor" href="#related-papers" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related Papers</h1>

<p><a href="http://arxiv.org/pdf/1602.07360v3.pdf">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5MB model size</a></p>

<p><a href="http://arxiv.org/pdf/1506.02626v3.pdf">Learning both Weights and Connections for Efficient Neural Network (NIPS'15)</a></p>

<p><a href="http://arxiv.org/pdf/1510.00149v5.pdf">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding (ICLR'16, best paper award)</a></p>

<p><a href="http://arxiv.org/pdf/1602.01528v1.pdf">EIE: Efficient Inference Engine on Compressed Deep Neural Network (ISCA'16)</a></p>

<p>If you find SqueezeNet and Deep Compression useful in your research, please consider citing the paper:</p>

<pre><code>@article{SqueezeNet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}

@article{DeepCompression,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={International Conference on Learning Representations (ICLR best paper award)},
  year={2016}
}

@inproceedings{han2015learning,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={1135--1143},
  year={2015}
}

@article{han2016eie,
  title={EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={International Conference on Computer Architecture (ISCA)},
  year={2016}
}
</code></pre>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/songhan/SqueezeNet-Deep-Compression">Squeezenet with Deep Compression</a> is maintained by <a href="https://github.com/songhan">songhan</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
